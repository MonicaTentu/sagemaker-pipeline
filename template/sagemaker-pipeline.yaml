---
AWSTemplateFormatVersion: 2010-09-09
Description: >
  CICD Pipeline for SageMaker Training
Parameters:
  Email:
    Description: The Email Address that notifications are sent to
    Type: String

  GitHubUser:
    Type: String
    Description: Your username or organization name on GitHub.

  GitHubRepo:
    Type: String
    Default: tf-gamesbiz
    Description: The repo name of the sample service.

  GitHubBranch:
    Type: String
    Default: dev
    Description: The branch of the repo to continuously deploy.

  GitHubToken:
    Type: String
    NoEcho: true
    Description: >
      Token for the user specified above. (https://github.com/settings/tokens)

  SyntheticBucketName:
    Type: String
    Description: >
      This is the bucket name for the bucket holding the synthetic data for models and is indexed
      by gitrepo name of each model e.g. <SyntheticBucketName>/<GitRepo>/<SomePartialKey>/*.csv
    Default: synthetic-data-bkt

  BuildTimeoutMins:
    Type: Number
    Description: Time out Limit in Minutes for CodeBuild
    Default: 30

  PythonBuildVersion:
    Default: aws/codebuild/python:3.6.5-1.3.2
    Type: String
    Description: Python Version for the Linux Container in CodeBuild
    AllowedValues:
      - aws/codebuild/python:3.6.5-1.3.2
      - aws/codebuild/python:3.6.5-1.3.2

  InstanceCount:
    Description: Number of Training Instances for SageMaker
    Type: Number
    Default: 1

  InstanceType:
    Description: Type of SageMaker Compute Instance
    Type: String
    Default: ml.m4.4xlarge

  MaxRuntimeInSeconds:
    Type: Number
    Default: 86400

  VolInGB:
    Description: This is the Volume in GB for each SageMaker Instance
    Type: Number
    Default: 30

Metadata:
  AWS::CloudFormation::Interface:
    ParameterLabels:
      Email:
        default: "Email for SNS Notification"

      GitHubUser:
        default: "GitHub Username"

      GitHubRepo:
        default: "GitHub Repository"

      GitHubBranch:
        default: "Repository Branch"

      GitHubToken:
        default: "Personal Access Token"

      PythonBuildVersion:
        default: "Select which version of Python applies to CodeBuild Project"

      BuildTimeoutMins:
        default: "Enter the Timeout limit (in minutes) for CodeBuild Project"

      InstanceCount:
        default: "Number of SageMaker Training Instances"

      InstanceType:
        default: "The Type of Instance for SageMaker Training Job"

      SyntheticBucketName:
        default: "The name for the S3 input bucket which holds synthetic data"

    ParameterGroups:
      - Label:
          default: Notification Configuration
        Parameters:
          - Email

      - Label:
          default: GitHub Configuration
        Parameters:
          - GitHubRepo
          - GitHubBranch
          - GitHubUser
          - GitHubToken

      - Label:
          default: Synthetic Data S3 Bucket Configuration
        Parameters:
          - SyntheticBucketName

      - Label:
          default: AWS CodeBuild Configuration
        Parameters:
          - PythonBuildVersion
          - BuildTimeoutMins

      - Label:
          default: Amazon SageMaker Configuration
        Parameters:
          - InstanceCount
          - InstanceType
          - VolInGB
          - MaxRuntimeInSeconds

Resources:
  MetaDataStore:
    Type: AWS::DynamoDB::Table
    Properties:
      AttributeDefinitions:
        - AttributeName: "training_job_name"
          AttributeType: "S"
      KeySchema:
        - AttributeName: "training_job_name"
          KeyType: "HASH"
      ProvisionedThroughput:
        ReadCapacityUnits: "10"
        WriteCapacityUnits: "5"

  Repository:
    Type: AWS::ECR::Repository
    DeletionPolicy: Retain

  ArtifactStoreBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties:
      BucketName: !Sub ${AWS::StackName}-artifact-store
      VersioningConfiguration:
        Status: Enabled

  ModelArtifactBucket:
    Type: AWS::S3::Bucket
    DependsOn: LambdaInvokeOutputPermission
    DeletionPolicy: Retain
    Properties:
      BucketName: !Sub ${AWS::StackName}-model-output
      VersioningConfiguration:
        Status: Enabled
      NotificationConfiguration:
        LambdaConfigurations:
          - Function: !GetAtt LambdaOutputTrigger.Arn
            Event: "s3:ObjectCreated:*"

  LambdaInvokeOutputPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt LambdaOutputTrigger.Arn
      Action: 'lambda:InvokeFunction'
      Principal: s3.amazonaws.com
      SourceAccount: !Ref AWS::AccountId
      SourceArn: !Sub arn:aws:s3:::${AWS::StackName}-model-output

  TrainingInputBucket:
    Type: AWS::S3::Bucket
    DependsOn: LambdaInvokePermission
    DeletionPolicy: Retain
    Properties:
      BucketName: !Sub ${AWS::StackName}-input-data
      VersioningConfiguration:
        Status: Enabled
      NotificationConfiguration:
        LambdaConfigurations:
          - Function: !GetAtt LambdaBuildTrigger.Arn
            Event: "s3:ObjectCreated:*"
            Filter:
              S3Key:
                Rules:
                  - Name: Prefix
                    Value: input/data/
                  - Name: Suffix
                    Value: hyperparameters.json

  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt LambdaBuildTrigger.Arn
      Action: 'lambda:InvokeFunction'
      Principal: s3.amazonaws.com
      SourceAccount: !Ref AWS::AccountId
      SourceArn: !Sub arn:aws:s3:::${AWS::StackName}-input-data

  PipelineEndSNSTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub ${AWS::StackName}-end-topic
      Subscription:
        - Endpoint: !Ref Email
          Protocol: email

  PipelineServiceRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::StackName}-pipeline-role
      AssumeRolePolicyDocument:
        Statement:
        - Action: ['sts:AssumeRole']
          Effect: Allow
          Principal:
            Service: [codepipeline.amazonaws.com]
        Version: '2012-10-17'
      Path: /
      Policies:
        - PolicyName: CodePipelineAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Action:
                - s3:*
                - codebuild:*
                - cloudformation:CreateStack
                - cloudformation:DescribeStacks
                - cloudformation:DeleteStack
                - cloudformation:UpdateStack
                - cloudformation:CreateChangeSet
                - cloudformation:ExecuteChangeSet
                - cloudformation:DeleteChangeSet
                - cloudformation:DescribeChangeSet
                - cloudformation:SetStackPolicy
                - lambda:*
                - iam:PassRole
                - sns:Publish
                Effect: Allow
                Resource: '*'

  CodeBuildServiceRole:
    Type: AWS::IAM::Role
    Properties:
      Path: /
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - codebuild.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Resource: "*"
                Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                  - ecr:GetAuthorizationToken
                  - ec2:*
                  - lambda:*
                  - iam:PassRole
              - Resource: !Sub arn:aws:s3:::${TrainingInputBucket}*
                Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:GetObjectVersion
                  - s3:ListObjects
              - Resource: !Sub arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/${Repository}
                Effect: Allow
                Action:
                  - ecr:GetDownloadUrlForLayer
                  - ecr:BatchGetImage
                  - ecr:BatchCheckLayerAvailability
                  - ecr:PutImage
                  - ecr:InitiateLayerUpload
                  - ecr:UploadLayerPart
                  - ecr:CompleteLayerUpload
      Path: /
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
        - arn:aws:iam::aws:policy/AWSCodeBuildAdminAccess
        - arn:aws:iam::aws:policy/AmazonS3FullAccess

  LambdaServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
                - sagemaker.amazonaws.com
                - codebuild.amazonaws.com
            Action:
              - sts:AssumeRole
      Policies:
        - PolicyName: LambdaPipelineRole
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Resource: "*"
                Effect: Allow
                Action:
                  - ec2:DescribeVpcs
                  - ec2:DescribeSubnets
                  - ec2:DescribeSecurityGroups
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:GetLogEvents
                  - logs:PutLogEvents
                  - ec2:CreateNetworkInterface
                  - ec2:DescribeNetworkInterfaces
                  - ec2:DeleteNetworkInterface
                  - ec2:DescribeDhcpOptions
                  - ec2:DetachNetworkInterface
                  - ec2:ModifyNetworkInterfaceAttribute
                  - iam:PassRole
                  - sns:Publish
                  - lambda:InvokeFunction
                  - cloudformation:DescribeStacks
                  - codepipeline:PutJobSuccessResult
                  - codepipeline:PutJobFailureResult
                  - codepipeline:GetJobDetails
                  - codepipeline:PutApprovalResult
                  - s3:*
      Path: /
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
        - arn:aws:iam::aws:policy/AWSCodeBuildAdminAccess
        - arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
        - arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess
        - arn:aws:iam::aws:policy/AmazonS3FullAccess
        - arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole

  CustomS3FolderHandler:
    Type: Custom::CustomS3FolderHandler
    DependsOn: TrainingInputBucket
    Properties:
      ServiceToken: !GetAtt LambdaS3FolderFunction.Arn
      SyntheticBucketName: !Ref SyntheticBucketName
      TrainingInputS3: !Ref TrainingInputBucket
      TopLevelFolder: !Ref GitHubRepo

  LambdaS3FolderFunction:
    Type: AWS::Lambda::Function
    Properties:
      Description: On stack creation sets up the Sagemaker equivalent folder structure in Training input S3 Bucket
      Handler: index.lambda_handler
      Runtime: python3.6
      Timeout: '300'
      Role: !GetAtt 'LambdaServiceRole.Arn'
      Code:
        ZipFile: |
          import json
          import os
          import boto3
          from botocore.vendored import requests
          import cfnresponse

          s3 = boto3.client('s3')

          def lambda_handler(event, context):

            input_bucket_name = str(event['ResourceProperties']['TrainingInputS3'])
            synthetic_bucket_name = str(event['ResourceProperties']['SyntheticBucketName'])
            top_level_folder = str(event['ResourceProperties']['TopLevelFolder'])

            if(event['RequestType'] == "Create"):

              synthetic_data_obj_res = s3.list_objects(Bucket=synthetic_bucket_name,
                Marker=top_level_folder + "/", Prefix=top_level_folder + "/")

              object_key_list = list()

              if 'Contents' not in synthetic_data_obj_res:
                print("synthetic-data-bucket empty for this repo")
              else:
                for key in synthetic_data_obj_res['Contents']:
                  object_key_name = key['Key'][ key['Key'].find("/") + 1 : ]
                  object_key_list.append(str(os.path.dirname(object_key_name)))

                for partial_key_name in list(set(object_key_list)):
                  if partial_key_name=='':
                    input_directory_path2 = "input/data/training" + "/"
                    input_directory_path3 = "input/data/validation" + "/"
                    input_directory_path4 = "input/data/testing" + "/"
                  else:
                    input_directory_path2 = "input/data/training" + "/" + partial_key_name + "/"
                    input_directory_path3 = "input/data/validation" + "/" + partial_key_name + "/"
                    input_directory_path4 = "input/data/testing" + "/" + partial_key_name + "/"

                  input_directory_path1 = "input/config" + "/"

                  s3.put_object(Bucket=input_bucket_name, Key=input_directory_path1 )
                  s3.put_object(Bucket=input_bucket_name, Key=input_directory_path2 )
                  s3.put_object(Bucket=input_bucket_name, Key=input_directory_path3 )
                  s3.put_object(Bucket=input_bucket_name, Key=input_directory_path4 )

              responseData = {}
              cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData)
            else:
              cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData=None)

  LambdaBuildTrigger:
    Type: AWS::Lambda::Function
    Properties:
      Description: Checks if Codebuild project exists and then manually executes it.
      Environment:
        Variables:
          'PIPELINE': !Ref 'AWS::StackName'
      Handler: index.lambda_handler
      Runtime: python3.6
      Timeout: '300'
      Role: !GetAtt 'LambdaServiceRole.Arn'
      Code:
        ZipFile: |
          import os
          import json
          import boto3
          import logging

          codepipeline = boto3.client('codepipeline')

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          def lambda_handler(event, context):
            logger.info('got event {}'.format(event))
            pipelineName = str(os.environ["PIPELINE"])
            response = codepipeline.list_pipelines()

            pipeline_list = []
            for pipe in response['pipelines']:
              pipeline_list.append(pipe['Name'])

            if pipelineName in pipline_list:
              response1 = codepipeline.list_pipeline_executions(pipelineName)

              if not response1['pipelineExecutionSummaries'][0]:
                print('Kick off a new pipeline execution')
                codepipeline.start_pipeline_execution(pipelineName)
              else:
                latest_execution = response1['pipelineExecutionSummaries'][0]
                execution_status = str(latest_execution['status'])

                if execution_status == 'InProgress':
                  print('Execution In Progress Do Nothing')

                if execution_status == 'Succeeded':
                  print('Restarting execution for Pipeline')
                  codepipeline.start_pipeline_execution(pipelineName)
            else:
              print("No Pipeline Execution Available")

  CodeBuildProject:
    Type: AWS::CodeBuild::Project
    Properties:
      Artifacts:
        Type: CODEPIPELINE
      Cache:
        Type: NO_CACHE
      Source:
        Type: CODEPIPELINE
        GitCloneDepth: 1
        BuildSpec: |
          version: 0.2
          phases:
            install:
              commands:
                - pip install --upgrade pip
            pre_build:
              commands:
                - export IMG="${REPO_NAME}"
                - export PAYLOAD={\"COMMIT_ID\":\"${CODEBUILD_RESOLVED_SOURCE_VERSION}\",\"IMG\":\"${REPO_NAME}\"}
                - echo "${PAYLOAD}"
                - echo "${PAYLOAD}" > outfile.txt
                - export entry_point=$(ps -ef | awk '/entry_point={/{flag=1;next}/}/{flag=0}flag { print $1 }' setup.py )
                - export module_name=$(echo ${entry_point} | cut -d ':' -f1 | sed -e 's/^"//' -e 's/"$//')
                - export function_name=$(echo ${entry_point} | cut -d ':' -f2 | sed -e 's/^"//' -e 's/"$//')
                - python setup.py egg_info
                - export pkg_name=$(ps -ef | awk '{ if ($1 ~ /Name:/) print $2}' *.egg-info/PKG-INFO)
                - export pkg_version=$(ps -ef | awk '{ if ($1 ~ /^Version:/) print $2}' *.egg-info/PKG-INFO)
                - export dependencies=""
                - |
                  while read -r line
                  do
                       dependencies="$dependencies""$line "
                  done < "${pkg_name}.egg-info/requires.txt"
                - python setup.py sdist
                - cd dist
                - echo "# ------------------------------------------------------------------------------" >> Dockerfile
                - echo "# Main" >> Dockerfile
                - echo "# =======" >> Dockerfile
                - echo "# Can be used as base image for regular training job executions" >> Dockerfile
                - echo "# ------------------------------------------------------------------------------" >> Dockerfile
                - echo "" >> Dockerfile
                - echo "FROM python:3.6-slim" >> Dockerfile
                - echo "" >> Dockerfile
                - echo "# Add standard build tools and libraries" >> Dockerfile
                - echo "RUN apt-get -y update && apt-get install -y --no-install-recommends \ " >> Dockerfile
                - echo "    build-essential \ " >> Dockerfile
                - echo "    unixodbc-dev \ " >> Dockerfile
                - echo "    libboost-python-dev \ " >> Dockerfile
                - echo "    ca-certificates" >> Dockerfile
                - echo "" >> Dockerfile
                - echo "# Install any package-specific python requirements" >> Dockerfile
                - echo "RUN pip install $dependencies" >> Dockerfile
                - echo "" >> Dockerfile
                - echo "# Copy over entrypoint script and define image entry"
                - echo "COPY run.py /opt/run.py" >> Dockerfile
                - echo "ENTRYPOINT ["'"python"'", "'"/opt/run.py"'"]" >> Dockerfile
                - echo "" >> Dockerfile
                - echo "# Copy and install training code" >> Dockerfile
                - echo "COPY $pkg_name-$pkg_version.tar.gz /opt/$pkg_name-$pkg_version.tar.gz" >> Dockerfile
                - echo "RUN pip install /opt/$pkg_name-$pkg_version.tar.gz" >> Dockerfile
                - echo "from $module_name import $function_name" >> run.py
                - echo "$function_name()" >> run.py
                - cd ..
            build:
              commands:
                - $(aws ecr get-login --region "${AWS_DEFAULT_REGION}" --no-include-email)
                - docker build -t "${IMG}" ./dist
                - docker tag "${IMG}" "${FULL_NAME}"
            post_build:
              commands:
                - docker push "${FULL_NAME}"
          artifacts:
            files:
              - outfile.txt
            discard-paths: yes
      Environment:
        ComputeType: BUILD_GENERAL1_LARGE
        Image: !Ref PythonBuildVersion
        Type: LINUX_CONTAINER
        PrivilegedMode: true
        EnvironmentVariables:
          - Name: AWS_DEFAULT_REGION
            Value: !Ref AWS::Region
          - Name: FULL_NAME
            Value: !Sub ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${Repository}:latest
          - Name: REPO_NAME
            Value: !Sub ${GitHubRepo}
      Name: !Ref AWS::StackName
      ServiceRole: !Ref CodeBuildServiceRole
      TimeoutInMinutes: !Ref BuildTimeoutMins

  LambdaCheckInput:
    Type: AWS::Lambda::Function
    Properties:
      Description: Checks if input data and config are present
      Environment:
        Variables:
          'ECR_REPO': !Ref Repository
          'SYNTHETIC_DATA_NAME': !Ref SyntheticBucketName
          'SRC_BKT_NAME': !Ref TrainingInputBucket
          'META_DATA_STORE': !Ref MetaDataStore
          'IMG': !Sub ${GitHubRepo}
      Handler: index.main
      Runtime: python3.6
      Timeout: '300'
      Role: !GetAtt 'LambdaServiceRole.Arn'
      Code:
        ZipFile: |
          import os
          import json
          import boto3
          import botocore
          import shutil
          import traceback

          code_pipeline = boto3.client('codepipeline')
          s3 = boto3.client('s3')

          def main(event, context):
            try:
              job_id = event['CodePipeline.job']['id']

              training_job_name = str(os.environ["IMG"])
              response3 = s3.list_objects(Bucket=str(os.environ["SRC_BKT_NAME"]), Marker='input/data/training/', Prefix='input/data/training/')
              response4 = s3.list_objects(Bucket=str(os.environ["SYNTHETIC_DATA_NAME"]), Marker=training_job_name + '/', Prefix=training_job_name + '/')

              if 'Contents' in response3:
                object_list3 = list()
                for key in response3['Contents']:
                  object_list3.append(os.path.basename(key['Key']))
                object_list4 = list(set(object_list3))
                if len(object_list4) >= 1:
                  if list(set(object_list4)) != ['']:
                    print("Training Data Already Exists")
                    code_pipeline.put_job_success_result(jobId=job_id)
                  else:
                    print("Copying synthetic bucket")
                    if 'Contents' in response4:
                      print("Copying From Synthetic Bucket")
                      for key in response4['Contents']:
                        s = str(key['Key']);
                        object_name = s[ s.find("/") + 1 : ]
                        response = s3.copy_object( Bucket=str(os.environ["SRC_BKT_NAME"]), CopySource={'Bucket': str(os.environ["SYNTHETIC_DATA_NAME"]), 'Key': s}, Key='input/data/training/'+ str(object_name))
                        code_pipeline.put_job_success_result(jobId=job_id)
                    else:
                      print('No Content in Synthetic Data Bucket')
                      code_pipeline.put_job_failure_result(jobId=job_id)
              else:
                print("Copying from synthetic bucket")
                if 'Contents' in response4:
                  print("Copying From Synthetic Bucket")
                  for key in response4['Contents']:
                    s = str(key['Key']);
                    object_name = s[ s.find("/") + 1 : ]
                    response = s3.copy_object( Bucket=str(os.environ["SRC_BKT_NAME"]), CopySource={'Bucket': str(os.environ["SYNTHETIC_DATA_NAME"]), 'Key': s}, Key='input/data/training/'+ str(object_name))
                    code_pipeline.put_job_success_result(jobId=job_id)
                else:
                  print('No Content in Synthetic Data Bucket')
                  code_pipeline.put_job_failure_result(jobId=job_id)
            except Exception as e:
              code_pipeline.put_job_failure_result(jobId=job_id, failureDetails={'message': e, 'type': 'JobFailed'})

  LambdaSageMakerTrigger:
    Type: AWS::Lambda::Function
    Properties:
      Description: Writes to DynamoDB and calls the SageMaker CreateTrainingJob API
      Environment:
        Variables:
          'ECR_REPO': !Ref Repository
          'SYNTHETIC_DATA_NAME': !Ref SyntheticBucketName
          'SRC_BKT_NAME': !Ref TrainingInputBucket
          'META_DATA_STORE': !Ref MetaDataStore
          'IMG': !Sub ${GitHubRepo}
          'FULL_NAME': !Sub ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${Repository}:latest
          'SAGE_ROLE_ARN': !GetAtt 'LambdaServiceRole.Arn'
          'INSTANCE_TYPE': !Ref InstanceType
          'INSTANCE_CNT': !Ref InstanceCount
          'EBS_VOL_GB': !Ref VolInGB
          'RUN_TIME_SEC': !Ref MaxRuntimeInSeconds
          'SRC_BKT_URI': !Sub s3://${TrainingInputBucket}/input/data/training/
          'DEST_BKT_URI': !Sub s3://${ModelArtifactBucket}/
      Handler: index.main
      Runtime: python3.6
      Timeout: '300'
      Role: !GetAtt 'LambdaServiceRole.Arn'
      Code:
        ZipFile: |
          import os, json, datetime, boto3, botocore, shutil, zipfile, tempfile, traceback; from boto3.session import Session; boto3.set_stream_logger(level=1); ArtifactFileName = "outfile.txt"; code_pipeline = boto3.client('codepipeline'); sagemaker = boto3.client('sagemaker'); dynamodb = boto3.client('dynamodb');

          def get_artifact(s3, bucketName, objectKey):
            with tempfile.NamedTemporaryFile() as tmp_file:
              s3.download_file(bucketName, objectKey, tmp_file.name)
              with zipfile.ZipFile(tmp_file.name, 'r') as zip:
                return zip.read(ArtifactFileName)

          def main(event, context):
            try:
              job_id = event['CodePipeline.job']['id']; job_data = event['CodePipeline.job']['data']; input_artifact = job_data['inputArtifacts'][0]; config = job_data['actionConfiguration']['configuration']; credentials = job_data['artifactCredentials']; from_bucket = input_artifact['location']['s3Location']['bucketName']; from_bucket = input_artifact['location']['s3Location']['bucketName']; from_key = input_artifact['location']['s3Location']['objectKey']; from_revision = input_artifact['revision']; key_id = credentials['accessKeyId']; key_secret = credentials['secretAccessKey']; session_token = credentials['sessionToken']; session = Session(aws_access_key_id=key_id, aws_secret_access_key=key_secret, aws_session_token=session_token); s3 = session.client('s3', config=botocore.client.Config(signature_version='s3v4') ); docs = get_artifact(s3, from_bucket, from_key)
              if (docs):
                docs_dict = json.loads( docs.decode("utf-8") ); git_hash = docs_dict['COMMIT_ID']

              s3 = boto3.client('s3'); response5 = s3.list_objects( Bucket=str(os.environ["SRC_BKT_NAME"]), Marker='input/config/', Prefix='input/config/' ); hyper_param_dict={"foo":"bar"}
              if 'Contents' in response5:
                object_list5 = list()
                for key in response5['Contents']:
                  object_list5.append(key['Key'])
                if len(object_list5) >= 1:
                  print("Hyperparameters Already Exists"); result = s3.get_object(Bucket=str(os.environ["SRC_BKT_NAME"]), Key=object_list5[0]); text = result["Body"].read().decode(); hyper_param_dict = json.loads(text)

              s3_key_version_dict = dict(); versions = s3.list_object_versions(Bucket = str(os.environ["SRC_BKT_NAME"]), Prefix = 'input/'); x = versions.get('Versions')
              for key in x:
                Key = key['Key']; VersionId = key['VersionId']; Size = key['Size']; IsLatest = key['IsLatest']; LastModified = key['LastModified']
                if Size != 0:
                  if IsLatest == True:
                    key_name = str(Key); version_id = { 'S' : str(VersionId) }; s3_key_version_dict.update({key_name:version_id});

              training_job_name = str(os.environ["IMG"]) + '-' + str(datetime.datetime.today()).replace(' ','-').replace(':', '-').rsplit('.')[0]; dynamodb.put_item(TableName=str(os.environ["META_DATA_STORE"]), Item={ 'training_job_name':{ 'S' : training_job_name}, 'git_hash':{'S' : str(git_hash)}, 's3_input': { 'M': s3_key_version_dict } })
              sagemaker.create_training_job( TrainingJobName=training_job_name, HyperParameters=hyper_param_dict, AlgorithmSpecification={ 'TrainingImage': str(os.environ["FULL_NAME"]), 'TrainingInputMode': 'File'}, RoleArn=str(os.environ["SAGE_ROLE_ARN"]), InputDataConfig=[{ 'ChannelName': 'training', 'DataSource': { 'S3DataSource': { 'S3DataType': 'S3Prefix', 'S3Uri': str(os.environ["SRC_BKT_URI"]) } } }], ResourceConfig={ 'InstanceType': str(os.environ["INSTANCE_TYPE"]), 'InstanceCount': int(os.environ["INSTANCE_CNT"]), 'VolumeSizeInGB': int(os.environ["EBS_VOL_GB"])}, OutputDataConfig={'S3OutputPath': str(os.environ["DEST_BKT_URI"]) }, StoppingCondition={ 'MaxRuntimeInSeconds': int(os.environ["RUN_TIME_SEC"]) })

              code_pipeline.put_job_success_result(jobId=job_id)
            except Exception as e:
              code_pipeline.put_job_failure_result(jobId=job_id, failureDetails={'message': e, 'type': 'JobFailed'})

  LambdaOutputTrigger:
    Type: AWS::Lambda::Function
    Properties:
      Description: This is triggered by Model Artifact being created in the output artifact bucket
      Environment:
        Variables:
          'META_DATA_STORE': !Ref MetaDataStore
          'PIPELINE_END_TOPIC': !Ref PipelineEndSNSTopic
      Handler: index.lambda_handler
      Runtime: python3.6
      Timeout: '300'
      Role: !GetAtt 'LambdaServiceRole.Arn'
      Code:
        ZipFile: |
          import os
          import json
          import boto3
          import logging

          s3 = boto3.client('s3')
          dynamodb = boto3.client('dynamodb')
          sagemaker = boto3.client('sagemaker')
          sns = boto3.client('sns')

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          def lambda_handler(event, context):

            logger.info('got event{}'.format(event))

            bkt_name = event['Records'][0]['s3']['bucket']['name']
            obj_key = event['Records'][0]['s3']['object']['key']
            version_id = event['Records'][0]['s3']['object']['versionId']
            training_job_name = obj_key[ : obj_key.find("/")]

            res = sagemaker.list_training_jobs()
            for job in res['TrainingJobSummaries']:
              if job['TrainingJobName'] == training_job_name:
                job_status = str(job['TrainingJobStatus'])
                job_creation_time = str(job['CreationTime'])
                job_end_time = str(job['TrainingEndTime'])

            dynamodb.update_item(
              TableName=str(os.environ['META_DATA_STORE']),
              Key={'training_job_name': {'S':training_job_name}},
              UpdateExpression="SET #job_creation_time= :val1, #job_end_time= :val2, #job_status= :val3",
              ExpressionAttributeNames={'#job_creation_time': 'job_creation_time', '#job_end_time': 'job_end_time', '#job_status':'job_status'},
              ExpressionAttributeValues={':val1':{ 'S':job_creation_time}, ':val2':{ 'S':job_end_time}, ':val3':{ 'S':job_status}}
            )
            sns.publish(
              TopicArn = str(os.environ['PIPELINE_END_TOPIC']),
              Subject = 'Model Artifact Uploaded: ' + training_job_name,
              Message = 'File was uploaded to bucket: ' + obj_key
            )

  Pipeline:
    Type: AWS::CodePipeline::Pipeline
    Properties:
      ArtifactStore:
        Location: !Ref 'ArtifactStoreBucket'
        Type: S3
      DisableInboundStageTransitions: []
      Name: !Ref 'AWS::StackName'
      RoleArn: !GetAtt [PipelineServiceRole, Arn]
      Stages:
        - Name: Source
          Actions:
            - Name: Source
              ActionTypeId:
                Category: Source
                Owner: ThirdParty
                Provider: GitHub
                Version: '1'
              Configuration:
                Owner: !Ref 'GitHubUser'
                Repo: !Ref 'GitHubRepo'
                Branch: !Ref 'GitHubBranch'
                OAuthToken: !Ref 'GitHubToken'
              OutputArtifacts:
                - Name: src
              RunOrder: '1'
        - Name: Build
          Actions:
            - Name: Build_Push
              ActionTypeId:
                Category: Build
                Owner: AWS
                Provider: CodeBuild
                Version: '1'
              Configuration:
                ProjectName: !Ref 'CodeBuildProject'
              InputArtifacts:
                - Name: src
              OutputArtifacts:
                - Name: bld
              RunOrder: '1'
        - Name: Validate_Inputs
          Actions:
            - Name: CheckData
              ActionTypeId:
                Category: Invoke
                Owner: AWS
                Provider: Lambda
                Version: '1'
              Configuration:
                FunctionName: !Ref 'LambdaCheckInput'
              InputArtifacts:
                - Name: bld
              OutputArtifacts: []
              RunOrder: '1'
        - Name: SageMaker_Training
          Actions:
            - Name: SageMakerTrigger
              ActionTypeId:
                Category: Invoke
                Owner: AWS
                Provider: Lambda
                Version: '1'
              Configuration:
                FunctionName: !Ref 'LambdaSageMakerTrigger'
              InputArtifacts:
                - Name: bld
              OutputArtifacts: []
              RunOrder: '1'

Outputs:
  PipelineUrl:
    Value: !Sub https://console.aws.amazon.com/codepipeline/home?region=${AWS::Region}#/view/${Pipeline}
    Description: CodePipeline URL

  ModelArtifactBucket:
    Description: >
      SageMaker Model Output S3 bucket
    Value: !GetAtt 'ModelArtifactBucket.Arn'

  TrainingInputBucket:
    Description: >
      SageMaker Training Input S3 bucket
    Value: !GetAtt 'TrainingInputBucket.Arn'

  MetaDataStore:
    Description: >
      DynamoDB Table tracking pipeline artifacts
    Value: !GetAtt 'ModelArtifactBucket.Arn'